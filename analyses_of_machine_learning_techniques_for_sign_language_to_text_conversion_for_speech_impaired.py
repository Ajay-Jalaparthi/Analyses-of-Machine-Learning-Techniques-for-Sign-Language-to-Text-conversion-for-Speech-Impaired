# -*- coding: utf-8 -*-
"""Analyses of Machine Learning Techniques for Sign Language to Text conversion for Speech Impaired.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mejczihOPcJXRZbwXsGVypUalTgi5WKW
"""

!pip install mediapipe==0.10.7

!pip install opencv-python scikit-learn matplotlib --quiet

import kagglehub

# Download latest version
path = kagglehub.dataset_download("grassknoted/asl-alphabet")

print("Path to dataset files:", path)

import os

dataset_path = "/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1"

for item in os.listdir(dataset_path):
    print(item)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_dir = "/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train/"

# Preprocess: normalize and augment slightly
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)
val_datagen = ImageDataGenerator(rescale=1./255)


img_size = (64, 64)
batch_size = 32

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

labels = list(train_generator.class_indices.keys())

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='sparse',  # <- CHANGED
    subset='training',
    shuffle=True
)

val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='sparse',  # <- CHANGED
    subset='validation'
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(29, activation='softmax')  # 29 = 26 letters + other signs like space, nothing, etc.
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(train_generator, validation_data=val_generator, epochs=5)

loss, acc = model.evaluate(val_generator)
print(f"Validation Accuracy: {acc:.2f}")

from IPython.display import display, Javascript
from google.colab.output import eval_js
from IPython.display import Image
import cv2
import numpy as np
import PIL.Image
import io

# JavaScript code to capture image
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'ðŸ“¸ Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        video.style.display = 'block';
        const stream = await navigator.mediaDevices.getUserMedia({video: true});

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize video
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for capture
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();

        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = io.BytesIO(base64.b64decode(data.split(',')[1]))
    with open(filename, 'wb') as f:
        f.write(binary.getbuffer())
    return filename

import base64

filename = take_photo()
print(f"Image saved to {filename}")

# Display the image
import IPython.display as display
display.display(PIL.Image.open(filename))

import mediapipe as mp

image = cv2.imread(filename)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

with mp_hands.Hands(static_image_mode=True, max_num_hands=1) as hands:
    result = hands.process(image_rgb)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

# Show annotated image
cv2.imwrite('hand_detected.jpg', image)
display.display(PIL.Image.open('hand_detected.jpg'))

import cv2
import numpy as np

img_path = "/content/photo.jpg"

img = cv2.imread(img_path)
img = cv2.resize(img, img_size)
img = img.astype('float32') / 255.0
img = np.expand_dims(img, axis=0)

pred = model.predict(img)
predicted_label = labels[np.argmax(pred)]

print("Predicted Sign Language Letter:", predicted_label)